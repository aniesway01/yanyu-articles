#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
build_all.py â€” å»ºç½® YanYu çŸ¥è­˜åº«å®Œæ•´éœæ…‹ç¶²ç«™ï¼ˆ4 åˆ†é¡ï¼‰
  1. å¾®ä¿¡æ–‡ç« åº«  2. ä¿éšªåˆ¤æ±ºåº«  3. Prompt åº«  4. AI ä½¿ç”¨æŠ€å·§
ç”¨æ³•ï¼špython build_all.py
"""

import json, os, re, sys, io, datetime, shutil, logging
from pathlib import Path
from urllib.parse import quote

if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

try:
    import markdown
except ImportError:
    print("pip install markdown"); sys.exit(1)

# â”€â”€â”€ Paths â”€â”€â”€
BASE       = Path(r"C:\AntiGravityFile\YanYuInc")
SITE       = BASE / "site"
LOG_DIR    = BASE / "logs"
WC_STATE   = BASE / "wechat_articles" / "_state.json"
WC_ARTS    = BASE / "wechat_articles" / "articles"
JDG_DIR    = BASE / "judgments"
JDG_EB_DIR = JDG_DIR / "ebookhub"
PROMPT_DIR = BASE / "prompts"
PROMPT_IDX = PROMPT_DIR / "_index.json"
PROMPT_YAML_DIR = PROMPT_DIR / "roles"
PROMPT_SRC = Path(r"C:\AntiGravityFile\Project\facebookPrompt\_legacy_prompt_collection")
EBOOK_SRC  = Path(r"C:\AntiGravityFile\Project\ebookhub\library")
TIPS_DIR   = BASE / "ai_tips"
TIPS_STATE = TIPS_DIR / "_state.json"
COMM_DIR   = BASE / "community_ai"
COMM_REPOS = COMM_DIR / "github_repos.json"
COMM_MSGS  = COMM_DIR / "_raw_ai_messages.json"


# â”€â”€â”€ Logging â”€â”€â”€
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_path = LOG_DIR / f"build_all_{datetime.datetime.now():%Y%m%d_%H%M%S}.log"
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.FileHandler(log_path, encoding="utf-8"),
              logging.StreamHandler(sys.stdout)])
log = logging.getLogger()

# â”€â”€â”€ Shared CSS â”€â”€â”€
CSS = """
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:-apple-system,"PingFang SC","Microsoft YaHei",sans-serif;background:#f5f5f5;color:#333;line-height:1.6}
.ctn{max-width:960px;margin:0 auto;padding:20px}
a{color:#1a73e8;text-decoration:none}a:hover{text-decoration:underline}
header{background:#1a73e8;color:#fff;padding:40px 20px;text-align:center;margin-bottom:30px;border-radius:8px}
header h1{font-size:28px;margin-bottom:8px}header p{opacity:.9;font-size:14px}
nav{margin-bottom:20px}nav a{font-size:14px}
footer{text-align:center;padding:30px;color:#999;font-size:13px}
.stats{display:flex;gap:15px;justify-content:center;margin-top:15px;flex-wrap:wrap}
.stat{background:rgba(255,255,255,.2);padding:5px 15px;border-radius:20px;font-size:13px}
.cats{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:20px;margin-bottom:30px}
.cc{background:#fff;border-radius:12px;padding:30px;text-align:center;cursor:pointer;box-shadow:0 2px 8px rgba(0,0,0,.1);transition:all .3s}
.cc:hover{transform:translateY(-4px);box-shadow:0 8px 24px rgba(0,0,0,.15)}
.cc-icon{font-size:48px;margin-bottom:15px}
.cc h2{font-size:20px;margin-bottom:8px}.cc p{font-size:14px;color:#666;margin-bottom:12px}
.cc .num{font-size:13px;color:#1a73e8;font-weight:500}
.card{background:#fff;border-radius:8px;padding:20px;box-shadow:0 1px 3px rgba(0,0,0,.1);margin-bottom:12px;transition:transform .2s}
.card:hover{transform:translateY(-2px);box-shadow:0 4px 12px rgba(0,0,0,.1)}
.card h3{font-size:17px;margin-bottom:8px}.card h3 a{color:#1a73e8}
.meta{font-size:13px;color:#666;margin-bottom:8px}
.dl a{color:#1a73e8;margin-right:12px;font-size:13px}
.search{width:100%;padding:10px 15px;border:1px solid #ddd;border-radius:6px;font-size:14px;margin-bottom:15px}
.tag{display:inline-block;padding:2px 8px;border-radius:10px;font-size:12px}
.tag-\u4fdd\u96aa\u7406\u8ce0{background:#e8f0fe;color:#1a73e8}
.tag-\u91ab\u7642\u5065\u5eb7{background:#e6f4ea;color:#137333}
.tag-AI\u79d1\u6280{background:#fce8e6;color:#c5221f}
.tag-\u884c\u696d\u5206\u6790{background:#fef7e0;color:#e37400}
.tag-\u751f\u6d3b\u5176\u4ed6{background:#f3e8fd;color:#7627bb}
.tag-è£å®šæ›¸{background:#fff3e0;color:#e65100}
.tag-åˆ¤æ±ºæ›¸{background:#e3f2fd;color:#1565c0}
.tag-æ–‡æ›¸{background:#f3e8fd;color:#7627bb}
.hidden{display:none!important}
.tg-btns{display:flex;gap:8px;flex-wrap:wrap;margin-bottom:15px}
.tg-btn{padding:5px 12px;border:1px solid #ddd;border-radius:15px;background:#fff;cursor:pointer;font-size:13px}
.tg-btn:hover,.tg-btn.active{background:#1a73e8;color:#fff;border-color:#1a73e8}
article{background:#fff;border-radius:8px;padding:30px;box-shadow:0 1px 3px rgba(0,0,0,.1)}
article h1{font-size:24px;margin-bottom:15px;line-height:1.4}
.content{font-size:16px;line-height:1.8}
.content img{max-width:100%;height:auto;border-radius:4px;margin:15px 0}
.content p{margin-bottom:15px}
.content h2,.content h3{margin:25px 0 10px}
.content blockquote{border-left:3px solid #1a73e8;padding-left:15px;color:#555;margin:15px 0}
.content table{border-collapse:collapse;width:100%;margin:15px 0}
.content th,.content td{border:1px solid #ddd;padding:8px 12px;text-align:left}
.content th{background:#f5f5f5}
.content pre{background:#f8f8f8;padding:15px;border-radius:6px;overflow-x:auto;margin:15px 0}
.content code{font-size:14px}
.dl-btn{display:inline-block;padding:6px 14px;background:#1a73e8;color:#fff;border-radius:5px;margin-right:8px;font-size:13px;text-decoration:none}
.dl-btn:hover{background:#1557b0;text-decoration:none}
.dl-btn.sec{background:#5f6368}.dl-btn.sec:hover{background:#3c4043}
.yg{margin-bottom:20px}
.yg h3{font-size:16px;color:#1a73e8;margin-bottom:10px}
.fi{display:flex;justify-content:space-between;align-items:center;padding:10px 15px;background:#fafafa;border-radius:6px;margin-bottom:6px;flex-wrap:wrap;gap:8px}
.fi .sz{font-size:12px;color:#999}
.sec-title{font-size:20px;margin:30px 0 15px;padding-bottom:8px;border-bottom:2px solid #1a73e8}
"""

def extract_case_number(raw):
    """å¾å®Œæ•´ case_no æå–æ‹¬è™Ÿå…§æ ¸å¿ƒæ¡ˆè™Ÿï¼Œå¦‚ 'æ±Ÿè‹çœ...ï¼ˆ2016ï¼‰è‹10æ°‘ç»ˆå­—ç¬¬2629å·...' -> '(2016)è‹10æ°‘ç»ˆå­—ç¬¬2629å·'"""
    # æ”¯æ´ç°¡é«”ã€Œå·ã€å’Œç¹é«”ã€Œè™Ÿã€
    m = re.search(r'[ï¼ˆ(]\d{4}[ï¼‰)][^å·è™Ÿ]*[å·è™Ÿ]', raw)
    if m:
        s = m.group(0)
        s = s.replace('ï¼ˆ', '(').replace('ï¼‰', ')').replace('è™Ÿ', 'å·')
        return s
    # fallback: æ¸…é™¤å°¾éƒ¨åƒåœ¾ ( | ç­‰)
    return re.sub(r'\s*\|.*$', '', raw).strip()

def esc(s):
    return str(s).replace("&","&amp;").replace("<","&lt;").replace(">","&gt;").replace('"',"&quot;")

def page(title, body, back=None, extra_css="", extra_js=""):
    n = f'<nav><a href="{back}">&larr; \u8fd4\u56de</a></nav>' if back else ''
    sc = f'<script>{extra_js}</script>' if extra_js else ''
    # BOM download script: intercept .md download links, prepend UTF-8 BOM
    bom_js = r"""<script>document.addEventListener('click',function(e){var a=e.target.closest('a[download]');if(!a)return;var h=a.getAttribute('href')||'';if(!h.endsWith('.md')&&!h.endsWith('.txt'))return;e.preventDefault();fetch(a.href).then(function(r){return r.text()}).then(function(t){var b=new Blob(['\uFEFF'+t],{type:'text/markdown;charset=utf-8'});var u=URL.createObjectURL(b);var d=document.createElement('a');d.href=u;d.download=h.split('/').pop();document.body.appendChild(d);d.click();document.body.removeChild(d);URL.revokeObjectURL(u)}).catch(function(){window.open(a.href)})})</script>"""
    return f"""<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>{esc(title)} - YanYu \u77e5\u8b58\u5eab</title>
<style>{CSS}{extra_css}</style></head><body><div class="ctn">
{n}{body}
<footer><p><a href="https://github.com/aniesway01/yanyu-articles">GitHub Repo</a> | YanYu \u77e5\u8b58\u5eab</p></footer>
</div>{bom_js}{sc}</body></html>"""

def wf(path, content):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)

def md2html(text):
    return markdown.markdown(text, extensions=["tables","fenced_code"])

# â•â•â•â•â•â•â•â• Phase 1: Prepare Content â•â•â•â•â•â•â•â•

def prepare_judgments():
    """Scan converted ebookhub markdown files (from convert_judgments.py + ocr_judgments.py)"""
    JDG_EB_DIR.mkdir(parents=True, exist_ok=True)
    eb_files = []
    # Scan existing converted markdown files (both text-extract and OCR)
    for mf in sorted(JDG_EB_DIR.glob("*.md")):
        sz = mf.stat().st_size
        yr_m = re.search(r'^(\d{4})_', mf.name)
        yr = yr_m.group(1) if yr_m else "unknown"
        eb_files.append({"year": yr, "slug": mf.stem, "name": mf.name, "size_kb": round(sz/1024, 1)})
        log.info(f"\u5224\u6c7a MD: {mf.name} ({sz/1024:.0f}KB)")
    # Check which scan-only PDFs now have OCR output
    converted_years = {ef["year"] for ef in eb_files}
    all_scan = [
        {"year": "2014", "name": "\u4e2d\u56fd\u6cd5\u96622014\u5e74\u5ea6\u6848\u4f8b_\u4fdd\u9669\u7ea0\u7eb7.pdf", "note": "\u6383\u63cf\u5716\u7247\uff0c\u7121\u6cd5\u63d0\u53d6\u6587\u5b57"},
        {"year": "2015", "name": "\u4e2d\u56fd\u6cd5\u96622015\u5e74\u5ea6\u6848\u4f8b\u4fdd\u9669\u7ea0\u7eb7\uff08291\u9801\uff09.pdf", "note": "\u6383\u63cf\u5716\u7247\uff0c\u7121\u6cd5\u63d0\u53d6\u6587\u5b57"},
        {"year": "2017", "name": "\u4e2d\u56fd\u6cd5\u96622017\u5e74\u5ea6\u6848\u4f8b \u4fdd\u9669.pdf", "note": "\u6383\u63cf\u5716\u7247\uff0c\u7121\u6cd5\u63d0\u53d6\u6587\u5b57"},
        {"year": "2021", "name": "\u4e2d\u56fd\u6cd5\u96622021\u5e74\u5ea6\u6848\u4f8b\uff1a\u4fdd\u9669\u7ea0\u7eb7.pdf", "note": "\u6383\u63cf\u5716\u7247\uff0c\u7121\u6cd5\u63d0\u53d6\u6587\u5b57"},
        {"year": "2024", "name": "15.\u4fdd\u9669\u7ea0\u7eb7.pdf", "note": "\u6383\u63cf\u5716\u7247\uff0c\u7121\u6cd5\u63d0\u53d6\u6587\u5b57"},
    ]
    # Only keep PDFs that don't yet have an OCR markdown output
    scan_only = []
    for s in all_scan:
        # Check if any eb_file starts with this year and relates to this PDF
        has_ocr = any(ef["name"].startswith(s["year"] + "_") and ef["size_kb"] > 5
                      for ef in eb_files
                      if ef["year"] == s["year"] and "ocr" not in ef["name"].lower()
                      # Also check by matching the PDF stem
                      ) or any(ef["name"].startswith(s["year"] + "_") for ef in eb_files if ef["year"] == s["year"])
        # More precise: check if the OCR output file exists
        pdf_stem = s["name"].replace(".pdf", "")
        ocr_out = JDG_EB_DIR / f"{s['year']}_{pdf_stem}.md"
        if not ocr_out.exists():
            # Also check by glob for any file starting with year that wasn't there before
            year_files = [ef for ef in eb_files if ef["year"] == s["year"]]
            if not year_files:
                scan_only.append(s)
            # else: already have a converted file for this year, skip
    log.info(f"\u5224\u6c7a: {len(eb_files)} \u500b\u5df2\u8f49\u63db, {len(scan_only)} \u500b\u7121\u6cd5\u63d0\u53d6")
    return eb_files, scan_only

def prepare_prompts():
    PROMPT_DIR.mkdir(parents=True, exist_ok=True)
    ebook_dir = PROMPT_DIR / "ebooks"
    ebook_dir.mkdir(exist_ok=True)
    materials = {
        "knowledge_base.md": PROMPT_SRC / "Artifacts" / "AI_Golden_Knowledge_Base.md",
        "url_report.md": PROMPT_SRC / "_Archive" / "AI_URL_Report.md",
        "compact_digest.md": PROMPT_SRC / "_Archive" / "AI_Compact_Digest.md",
        "rich_digest.md": PROMPT_SRC / "_Archive" / "AI_Rich_Digest.md",
        "related_data.md": PROMPT_SRC / "_Archive" / "AI_Related_Data.md",
        "group_list.md": PROMPT_SRC / "grouplist.md",
    }
    copied = []
    for dest_name, src in materials.items():
        if src.exists():
            dest = PROMPT_DIR / dest_name
            if not dest.exists() or src.stat().st_mtime > dest.stat().st_mtime:
                shutil.copy2(src, dest)
            copied.append({"slug": dest_name, "size_kb": round(src.stat().st_size / 1024, 1)})
            log.info(f"Prompt: {dest_name} ({src.stat().st_size/1024:.0f}KB)")
        else:
            log.warning(f"\u627e\u4e0d\u5230: {src}")
    # Ebooks â€” pick largest file per slug to avoid duplicates
    ebook_candidates = {}  # slug -> (path, size)
    if EBOOK_SRC.exists():
        for d in EBOOK_SRC.iterdir():
            if not d.is_dir():
                continue
            nm = d.name.lower()
            if "prompt" not in nm and "\u63d0\u793a\u5de5\u7a0b" not in nm:
                continue
            md_dir = d / "md"
            if not md_dir.exists():
                continue
            for mf in md_dir.glob("*.md"):
                if "bible" in nm: slug = "prompt_bible.md"
                elif "handbook" in nm and "\u63d0\u793a" not in nm: slug = "prompt_handbook.md"
                elif "ultimate" in nm or "mastering" in nm: slug = "prompt_guide.md"
                elif "\u63d0\u793a" in nm or "\u624b\u518c" in nm: slug = "prompt_handbook_zh.md"
                else: slug = re.sub(r'[^\w]', '_', d.name)[:40] + ".md"
                sz = mf.stat().st_size
                if slug not in ebook_candidates or sz > ebook_candidates[slug][1]:
                    ebook_candidates[slug] = (mf, sz)
    for slug, (mf, sz) in ebook_candidates.items():
        dest = ebook_dir / slug
        if not dest.exists() or mf.stat().st_mtime > dest.stat().st_mtime:
            shutil.copy2(mf, dest)
        copied.append({"slug": f"ebooks/{slug}", "size_kb": round(sz / 1024, 1)})
        log.info(f"Ebook: {slug} ({sz/1024:.0f}KB)")
    log.info(f"Prompt \u7d20\u6750: {len(copied)} \u500b")
    return copied

# â•â•â•â•â•â•â•â• Phase 2: Build Site â•â•â•â•â•â•â•â•

def build_main_page(wc_n, wc_img, jdg_n, prm_n, tip_n, comm_n):
    body = f"""
<header>
  <h1>YanYu \u77e5\u8b58\u5eab</h1>
  <p>\u4fdd\u96aa\u7406\u8ce0\u8aee\u8a62 \u00b7 \u5224\u6c7a\u7814\u7a76 \u00b7 AI \u63d0\u793a\u8a5e \u00b7 AI \u5de5\u5177\u6559\u5b78</p>
  <div class="stats"><span class="stat">\u66f4\u65b0\u65bc {datetime.datetime.now():%Y-%m-%d}</span></div>
</header>
<div class="cats">
  <div class="cc" onclick="location.href='wechat.html'">
    <div class="cc-icon">\U0001f4f0</div><h2>\u5fae\u4fe1\u6587\u7ae0\u5eab</h2>
    <p>ShawnCH \u8f49\u767c\u7684\u4fdd\u96aa\u7406\u8ce0\u3001\u91ab\u7642\u5065\u5eb7\u76f8\u95dc\u6587\u7ae0</p>
    <div class="num">{wc_n} \u7bc7\u6587\u7ae0 \u00b7 {wc_img} \u5f35\u5716\u7247</div>
  </div>
  <div class="cc" onclick="location.href='judgments.html'">
    <div class="cc-icon">\u2696\ufe0f</div><h2>\u4fdd\u96aa\u5224\u6c7a\u5eab</h2>
    <p>\u4e2d\u570b\u6cd5\u9662\u4fdd\u96aa\u7cfe\u7d1b\u5224\u6c7a\u6848\u4f8b\u96c6\uff082014-2025\uff09</p>
    <div class="num">{jdg_n} \u4efd\u5224\u6c7a\u8cc7\u6599</div>
  </div>
  <div class="cc" onclick="location.href='prompts.html'">
    <div class="cc-icon">\U0001f916</div><h2>Prompt \u5eab</h2>
    <p>AI \u63d0\u793a\u8a5e\u6848\u4f8b\u3001\u6559\u5b78\u8cc7\u6e90\u8207\u96fb\u5b50\u66f8</p>
    <div class="num">{prm_n} \u4efd\u8cc7\u6599</div>
  </div>
  <div class="cc" onclick="location.href='ai-tips.html'">
    <div class="cc-icon">\U0001f4a1</div><h2>AI \u4f7f\u7528\u6280\u5de7</h2>
    <p>AI \u5de5\u5177\u6559\u5b78\u6587\u7ae0\uff08\u96fb\u8166\u738b\u963f\u9054\u7b49\uff09</p>
    <div class="num">{tip_n} \u7bc7\u6587\u7ae0</div>
  </div>
  <div class="cc" onclick="location.href='community-ai.html'">
    <div class="cc-icon">\U0001f4ac</div><h2>AI \u5fc3\u5f97\u793e\u7fa4\u5206\u4eab</h2>
    <p>\u4e09\u5144\u5f1f\u7fa4 GitHub / AI \u8cc7\u8a0a\u5f59\u6574</p>
    <div class="num">{comm_n} \u500b GitHub Repo</div>
  </div>
</div>"""
    wf(SITE / "index.html", page("\u9996\u9801", body))
    log.info("\u5efa\u7f6e: \u9996\u9801 index.html")

def build_wechat():
    with open(WC_STATE, "r", encoding="utf-8") as f:
        state = json.load(f)
    articles = sorted([a for a in state["articles"] if a["status"] == "fetched"],
                      key=lambda x: x.get("chat_time", ""), reverse=True)
    total_imgs, cards, tag_set = 0, [], set()
    for art in articles:
        slug = art.get("slug", "")
        if not slug: continue
        md_file = WC_ARTS / slug / "index.md"
        if not md_file.exists(): continue
        with open(md_file, "r", encoding="utf-8") as f:
            md_text = f.read()
        ch = md2html(md_text)
        ch = ch.replace('src="assets/', f'src="../../wechat_articles/articles/{slug}/assets/')
        ch = re.sub(r'<h1>.*?</h1>\s*<blockquote>.*?</blockquote>\s*<hr\s*/?>', '', ch, count=1, flags=re.DOTALL)
        title = art.get("real_title", art.get("title", "untitled"))
        author = art.get("author", "\u672a\u77e5")
        tags = art.get("tags", [])
        ct = art.get("chat_time", "")
        url = art.get("url", "")
        for t in tags: tag_set.add(t)
        th = " ".join(f'<span class="tag tag-{t}">{t}</span>' for t in tags)
        assets = WC_ARTS / slug / "assets"
        if assets.exists(): total_imgs += len(list(assets.iterdir()))
        # Article page
        ab = f"""<article><h1>{esc(title)}</h1>
<div class="meta"><strong>\u4f5c\u8005</strong>\uff1a{esc(author)} | <strong>\u8f49\u767c\u6642\u9593</strong>\uff1a{ct}\uff08by ShawnCH\uff09<br>
<a href="{url}" target="_blank">\u67e5\u770b\u539f\u6587</a><div style="margin-top:8px">{th}</div></div>
<div class="content">{ch}</div>
<div style="margin-top:25px;padding-top:20px;border-top:1px solid #eee"><h3 style="font-size:15px;margin-bottom:10px">\u4e0b\u8f09</h3>
<a class="dl-btn" href="../../wechat_articles/articles/{slug}/index.md" download>Markdown</a>
<a class="dl-btn sec" href="../../wechat_articles/articles/{slug}/article.txt" download>\u7d14\u6587\u5b57</a></div></article>"""
        wf(SITE / slug / "index.html", page(title, ab, back="../wechat.html"))
        cards.append(f'<div class="card" data-title="{esc(title)}" data-tags="{",".join(tags)}">'
            f'<h3><a href="{slug}/index.html">{esc(title)}</a></h3>'
            f'<div class="meta">{ct} | {esc(author)}</div><div>{th}</div>'
            f'<div class="dl"><a href="../wechat_articles/articles/{slug}/index.md" download>MD</a>'
            f' <a href="../wechat_articles/articles/{slug}/article.txt" download>TXT</a>'
            f' <a href="{url}" target="_blank">\u539f\u6587</a></div></div>')
    tg = '<button class="tg-btn active" onclick="ft(this,\'\')">\u5168\u90e8</button>\n'
    for t in sorted(tag_set):
        tg += f'<button class="tg-btn" onclick="ft(this,\'{t}\')">{t}</button>\n'
    body = f"""<header><h1>\U0001f4f0 \u5fae\u4fe1\u6587\u7ae0\u5eab</h1>
<p>ShawnCH\uff08\u4f55\u667a\u7fd4\uff09\u8f49\u767c\u7684\u4fdd\u96aa\u7406\u8ce0\u3001\u91ab\u7642\u5065\u5eb7\u76f8\u95dc\u6587\u7ae0</p>
<div class="stats"><span class="stat">\u5171 {len(articles)} \u7bc7\u6587\u7ae0</span><span class="stat">{total_imgs} \u5f35\u5716\u7247</span></div></header>
<input class="search" type="text" id="q" placeholder="\u641c\u5c0b\u6587\u7ae0\u6a19\u984c..." oninput="fa()">
<div class="tg-btns">{tg}</div><div id="arts">{"".join(cards)}</div>"""
    js = "let ct='';function ft(b,t){ct=t;document.querySelectorAll('.tg-btn').forEach(x=>x.classList.remove('active'));b.classList.add('active');fa();}function fa(){const q=document.getElementById('q').value.toLowerCase();document.querySelectorAll('.card').forEach(c=>{const m=(!q||c.dataset.title.toLowerCase().includes(q))&&(!ct||c.dataset.tags.includes(ct));c.classList.toggle('hidden',!m);});}"
    wf(SITE / "wechat.html", page("\u5fae\u4fe1\u6587\u7ae0\u5eab", body, back="index.html", extra_js=js))
    log.info(f"\u5efa\u7f6e: \u5fae\u4fe1\u6587\u7ae0\u5eab ({len(articles)} \u7bc7, {total_imgs} \u5716)")
    return len(articles), total_imgs

def build_judgments(eb_files, scan_only):
    CASE_IDX = JDG_DIR / "cases" / "_index.json"
    CASE_DIR = JDG_DIR / "cases"
    WENSHU_URLS_FILE = JDG_DIR / "_wenshu_urls.json"
    WENSHU_SEARCH = "https://wenshu.court.gov.cn/website/wenshu/181217BMTKHNT2W0/index.html?s21="

    # A. è®€å–è£åˆ¤æ–‡æ›¸ç¶² URL æ•¸æ“š
    wenshu_urls = {}
    if WENSHU_URLS_FILE.exists():
        with open(WENSHU_URLS_FILE, "r", encoding="utf-8") as f:
            wu_data = json.load(f)
        wenshu_urls = wu_data.get("urls", {})
        log.info(f"è£åˆ¤æ–‡æ›¸ç¶² URL: å·²è¼‰å…¥ {len(wenshu_urls)} ç­†")

    # B. è®€å– NotebookLM ç”Ÿæˆçš„ artifacts
    NLM_DIR = JDG_DIR / "_nlm_output"
    NLM_PROGRESS = NLM_DIR / "_progress.json"
    nlm_completed = {}
    if NLM_PROGRESS.exists():
        with open(NLM_PROGRESS, "r", encoding="utf-8") as f:
            nlm_data = json.load(f)
        nlm_completed = nlm_data.get("completed", {})
        log.info(f"NotebookLM artifacts: å·²è¼‰å…¥ {len(nlm_completed)} ç­†")

    # C. è®€å– AI åˆ†æçµæœï¼ˆæ‘˜è¦/é‡é»/æ³•å­¸è¦‹è§£ï¼‰
    AI_DIR = JDG_DIR / "_ai_analysis"
    ai_analyses = {}
    if AI_DIR.exists():
        for aj in AI_DIR.rglob("*.json"):
            if aj.name == "_progress.json":
                continue
            # Key: relative path without .json, e.g. "cases/2018/01_xxx" or "featured/001_xxx"
            rel = aj.relative_to(AI_DIR).with_suffix("").as_posix()
            try:
                with open(aj, "r", encoding="utf-8") as f:
                    ai_analyses[rel] = json.load(f)
            except Exception:
                pass
        log.info(f"AI åˆ†æ: å·²è¼‰å…¥ {len(ai_analyses)} ç­†")

    def build_analysis_section(ai_key):
        """Build HTML for AI-generated summary/key_points/legal_insights."""
        if ai_key not in ai_analyses:
            return ""
        a = ai_analyses[ai_key]
        parts = []
        # æ‘˜è¦
        if a.get("summary"):
            parts.append(f'<div class="analysis-block"><h2 class="sec-title">æ‘˜è¦</h2>'
                         f'<p style="font-size:15px;line-height:1.8">{esc(a["summary"])}</p></div>')
        # é‡é»
        kps = a.get("key_points", [])
        if kps:
            li = "".join(f"<li>{esc(k)}</li>" for k in kps)
            parts.append(f'<div class="analysis-block"><h2 class="sec-title">çˆ­è­°ç„¦é»èˆ‡è£åˆ¤è¦æ—¨</h2>'
                         f'<ul style="padding-left:20px;line-height:2">{li}</ul></div>')
        # æ³•å­¸è¦‹è§£
        if a.get("legal_insights"):
            # Split by newlines for paragraph formatting
            paras = a["legal_insights"].replace("\n\n", "\n").split("\n")
            body = "".join(f"<p>{esc(p)}</p>" for p in paras if p.strip())
            parts.append(f'<div class="analysis-block"><h2 class="sec-title">æ³•å­¸è¦‹è§£</h2>'
                         f'<div style="font-size:15px;line-height:1.8">{body}</div></div>')
        if not parts:
            return ""
        return "\n".join(parts)

    def build_nlm_section(nlm_key, detail_page_dir):
        """Build HTML for NotebookLM artifacts and copy files to site."""
        if nlm_key not in nlm_completed:
            return ""
        info = nlm_completed[nlm_key]
        nlm_src = NLM_DIR / nlm_key
        if not nlm_src.exists():
            return ""

        parts = []
        # Copy and reference infographic
        ig_src = nlm_src / "infographic.png"
        if ig_src.exists() and info.get("infographic"):
            ig_dest = detail_page_dir / "infographic.png"
            ig_dest.parent.mkdir(parents=True, exist_ok=True)
            import shutil
            shutil.copy2(ig_src, ig_dest)
            parts.append('<div style="margin:20px 0"><h3 style="font-size:15px;margin-bottom:10px">\U0001f4ca NotebookLM \u5206\u6790\u5716</h3>'
                         '<img src="infographic.png" alt="NotebookLM Infographic" '
                         'style="max-width:100%;border:1px solid #eee;border-radius:8px"></div>')

        # Copy and reference slides PDF
        sl_src = nlm_src / "slides.pdf"
        if sl_src.exists() and info.get("slides"):
            sl_dest = detail_page_dir / "slides.pdf"
            sl_dest.parent.mkdir(parents=True, exist_ok=True)
            import shutil
            shutil.copy2(sl_src, sl_dest)
            parts.append('<a class="dl-btn" href="slides.pdf" target="_blank">\U0001f4dd \u4e0b\u8f09 PPT \u7c21\u5831</a> ')

        # Copy mind map
        mm_src = nlm_src / "mindmap.json"
        if mm_src.exists() and info.get("mind_map"):
            mm_dest = detail_page_dir / "mindmap.json"
            mm_dest.parent.mkdir(parents=True, exist_ok=True)
            import shutil
            shutil.copy2(mm_src, mm_dest)
            parts.append('<a class="dl-btn sec" href="mindmap.json" download>\U0001f9e0 Mind Map JSON</a>')

        if not parts:
            return ""
        return '<div style="margin-top:25px;padding-top:20px;border-top:1px solid #eee">' + '\n'.join(parts) + '</div>'

    def get_wenshu_url(case_no_raw):
        """æ ¹æ“šæ¡ˆè™Ÿç²å–è£åˆ¤æ–‡æ›¸ç¶² URLï¼ˆçœŸå¯¦ URL æˆ–æœç´¢é€£çµï¼‰"""
        cn = extract_case_number(case_no_raw)
        if cn in wenshu_urls:
            return wenshu_urls[cn].get("wenshu_url", WENSHU_SEARCH + quote(cn))
        return WENSHU_SEARCH + quote(cn)

    def get_doc_type(title):
        """å¾æ¨™é¡Œæå–æ–‡æ›¸é¡å‹"""
        if "è£å®šæ›¸" in title or "è£å®šä¹¦" in title:
            return "è£å®šæ›¸"
        if "åˆ¤æ±ºæ›¸" in title or "åˆ¤å†³ä¹¦" in title:
            return "åˆ¤æ±ºæ›¸"
        return "æ–‡æ›¸"

    # 1. Original curated judgments (è£åˆ¤æ–‡æ›¸ç¶²ç²¾é¸)
    md_jdgs = []
    for mf in sorted(JDG_DIR.glob("*.md")):
        with open(mf, "r", encoding="utf-8") as f:
            text = f.read()
        m = re.search(r'^#\s+(.+)', text)
        title = m.group(1) if m else mf.stem
        m2 = re.search(r'\u6848\u865f[^\|]*\|\s*(.+)', text)
        case_no = m2.group(1).strip() if m2 else ""
        slug = mf.stem
        md_jdgs.append({"slug": slug, "title": title, "case_no": case_no, "text": text})
    for j in md_jdgs:
        ch = md2html(j["text"])
        w_url = get_wenshu_url(j["case_no"])
        detail_dir = SITE / "judgment" / j["slug"]
        nlm_key = f'featured/{j["slug"]}'
        nlm_html = build_nlm_section(nlm_key, detail_dir)
        ai_key = f'featured/{j["slug"]}'
        analysis_html = build_analysis_section(ai_key)
        ab = f'''<article>
<div style="margin-bottom:15px">
  <a class="dl-btn" href="{esc(w_url)}" target="_blank">è£åˆ¤æ–‡æ›¸ç¶²åŸæ–‡</a>
  <a class="dl-btn sec" href="../../../judgments/{quote(j["slug"])}.md" download>ä¸‹è¼‰ MD</a>
</div>
{analysis_html}
{nlm_html}
<div class="content" style="margin-top:20px;border-top:1px solid #eee;padding-top:15px">{ch}</div></article>'''
        wf(detail_dir / "index.html", page(j["title"], ab, back="../../judgments.html"))

    # 2. Individual cases from split (ä¸€æ¡ˆä¸€æª”)
    split_cases = []
    cases_by_year = {}
    if CASE_IDX.exists():
        with open(CASE_IDX, "r", encoding="utf-8") as f:
            cidx = json.load(f)
        split_cases = cidx.get("cases", [])
        for c in split_cases:
            cases_by_year.setdefault(c["year"], []).append(c)
        # Build individual case pages
        for c in split_cases:
            case_path = CASE_DIR / c["year"] / c["filename"]
            if not case_path.exists():
                continue
            with open(case_path, "r", encoding="utf-8") as f:
                text = f.read()
            ch = md2html(text)
            case_slug = f'{c["year"]}_{c["num"]:0>2}'
            detail_dir = SITE / "judgment" / case_slug
            # NLM key for annual cases
            case_stem = Path(c["filename"]).stem
            nlm_key = f'cases/{c["year"]}/{case_stem}'
            nlm_html = build_nlm_section(nlm_key, detail_dir)
            ai_key = f'cases/{c["year"]}/{case_stem}'
            analysis_html = build_analysis_section(ai_key)
            w_url = get_wenshu_url(c.get("case_no", ""))
            ab = f'''<article>
<div style="margin-bottom:15px">
  <a class="dl-btn" href="{esc(w_url)}" target="_blank">è£åˆ¤æ–‡æ›¸ç¶²åŸæ–‡</a>
  <a class="dl-btn sec" href="../../../judgments/cases/{c["year"]}/{quote(c["filename"])}" download>ä¸‹è¼‰ MD</a>
</div>
{analysis_html}
{nlm_html}
<details style="margin-top:25px;border-top:1px solid #eee;padding-top:15px">
<summary style="cursor:pointer;font-size:16px;font-weight:600;color:#1a73e8;padding:10px 0">
å±•é–‹åˆ¤æ±ºåŸæ–‡</summary>
<div class="content" style="margin-top:15px">{ch}</div>
</details></article>'''
            wf(detail_dir / "index.html",
               page(c["title"] or f"æ¡ˆä¾‹{c['num']}", ab, back="../../judgments.html"))

    # 3. Build listing page
    # 3a. ç²¾é¸åˆ¤æ±ºå¡ç‰‡
    md_cards = ""
    for j in md_jdgs:
        doc_type = get_doc_type(j["title"])
        w_url = get_wenshu_url(j["case_no"])
        nlm_key = f'featured/{j["slug"]}'
        has_nlm = nlm_key in nlm_completed
        nlm_badge = ' <span style="font-size:11px;background:#e8f5e9;color:#2e7d32;padding:1px 6px;border-radius:3px">NLM</span>' if has_nlm else ''
        md_cards += f'''<div class="card"><h3><a href="judgment/{j["slug"]}/index.html">{esc(j["title"])}</a></h3>
<div class="meta"><span class="tag tag-{doc_type}">{doc_type}</span> {esc(j["case_no"])}{nlm_badge}</div>
<div class="dl">
  <a href="{esc(w_url)}" target="_blank" class="dl-btn" style="font-size:12px;padding:3px 10px">è£åˆ¤æ–‡æ›¸ç¶²åŸæ–‡</a>
  <a href="../judgments/{quote(j["slug"])}.md" download style="font-size:12px;color:#666">MD</a>
</div></div>\n'''

    # 3b. å¹´åº¦æ¡ˆä¾‹ï¼ˆä¸€æ¡ˆä¸€æª”å¡ç‰‡ï¼‰
    case_cards = ""
    year_btns = '<button class="tg-btn active" onclick="fy(this,\'\')">å…¨éƒ¨</button>\n'
    for yr in sorted(cases_by_year):
        cnt = len(cases_by_year[yr])
        year_btns += f'<button class="tg-btn" onclick="fy(this,\'{yr}\')">{yr} ({cnt})</button>\n'
        for c in cases_by_year[yr]:
            case_slug = f'{c["year"]}_{c["num"]:0>2}'
            case_stem = Path(c["filename"]).stem
            title_s = esc(c["title"] or f"æ¡ˆä¾‹{c['num']}")
            parties_s = esc(c.get("parties", ""))
            case_no_s = esc(c.get("case_no", ""))
            w_url = get_wenshu_url(c.get("case_no", ""))
            search_text = f'{c["title"]} {c.get("parties","")} {c.get("case_type","")}'.lower()
            ai_key = f'cases/{c["year"]}/{case_stem}'
            has_ai = ai_key in ai_analyses
            ai_badge = ' <span style="font-size:11px;background:#e3f2fd;color:#1565c0;padding:1px 6px;border-radius:3px">AI</span>' if has_ai else ''
            # Show AI summary preview on card
            ai_preview = ""
            if has_ai:
                s = ai_analyses[ai_key].get("summary", "")
                if s:
                    ai_preview = f'<div style="font-size:13px;color:#555;margin-top:5px;line-height:1.5">{esc(s[:120])}{"..." if len(s)>120 else ""}</div>'
            case_cards += f'''<div class="card" data-year="{c["year"]}" data-search="{esc(search_text)}">
<h3><a href="judgment/{case_slug}/index.html">{title_s}</a></h3>
<div class="meta">{parties_s}{ai_badge}</div>
<div class="meta" style="color:#999">{case_no_s}</div>{ai_preview}
<div class="dl">
  <a href="{esc(w_url)}" target="_blank" style="font-size:12px">è£åˆ¤æ–‡æ›¸ç¶²</a>
</div></div>\n'''

    case_js = """let cy='';function fy(b,y){cy=y;document.querySelectorAll('.tg-btn').forEach(x=>x.classList.remove('active'));b.classList.add('active');fj();}
function fj(){const q=document.getElementById('jq').value.toLowerCase();document.querySelectorAll('#jcards .card').forEach(c=>{const m=(!q||c.dataset.search.includes(q))&&(!cy||c.dataset.year===cy);c.classList.toggle('hidden',!m);});}"""

    total_cases = len(split_cases)
    total = len(md_jdgs) + total_cases
    body = f"""<header><h1>ä¿éšªåˆ¤æ±ºåº«</h1>
<p>ä¸­åœ‹æ³•é™¢ä¿éšªç³¾ç´›åˆ¤æ±ºæ¡ˆä¾‹é›†ï¼ˆ2014-2025ï¼‰</p>
<div class="stats"><span class="stat">{len(md_jdgs)} ä»½ç²¾é¸åˆ¤æ±º</span><span class="stat">{total_cases} å€‹å¹´åº¦æ¡ˆä¾‹ï¼ˆä¸€æ¡ˆä¸€æª”ï¼‰</span><span class="stat">ä¾†æºï¼šä¸­åœ‹è£åˆ¤æ–‡æ›¸ç¶² + ä¸­åœ‹æ³•é™¢å¹´åº¦æ¡ˆä¾‹å¢æ›¸</span></div></header>
<h2 class="sec-title">è£åˆ¤æ–‡æ›¸ç¶² Â· ç²¾é¸åˆ¤æ±ºåˆ†æ</h2>
<p style="margin-bottom:15px;color:#666;font-size:14px">ä¾†æºï¼šä¸­åœ‹è£åˆ¤æ–‡æ›¸ç¶²ï¼Œäººèº«ä¿éšªåˆåŒç³¾ç´›æ¡ˆä»¶</p>
{md_cards}
<h2 class="sec-title">ä¸­åœ‹æ³•é™¢å¹´åº¦æ¡ˆä¾‹ Â· ä¿éšªç³¾ç´›ï¼ˆä¸€æ¡ˆä¸€æª”ï¼‰</h2>
<p style="margin-bottom:15px;color:#666;font-size:14px">ä¾†æºï¼šã€Šä¸­åœ‹æ³•é™¢å¹´åº¦æ¡ˆä¾‹ã€‹å¢æ›¸ï¼ˆ2018-2025ï¼‰ï¼Œæ¯å€‹æ¡ˆä¾‹ç¨ç«‹æˆæª”ï¼Œå«æ¡ˆä»¶åŸºæœ¬ä¿¡æ¯ã€æ¡ˆæƒ…ã€è£åˆ¤è¦æ—¨ã€æ³•å®˜åèª</p>
<input class="search" type="text" id="jq" placeholder="æœå°‹æ¡ˆä¾‹æ¨™é¡Œã€ç•¶äº‹äººã€æ¡ˆç”±..." oninput="fj()">
<div class="tg-btns">{year_btns}</div>
<div id="jcards">{case_cards}</div>"""
    wf(SITE / "judgments.html", page("ä¿éšªåˆ¤æ±ºåº«", body, back="index.html", extra_js=case_js))
    log.info(f"å»ºç½®: åˆ¤æ±ºåº« ({len(md_jdgs)} ç²¾é¸ + {total_cases} å¹´åº¦æ¡ˆä¾‹)")
    return total

def build_prompts(prompt_files):
    # â”€â”€â”€ Part A: 662 YAML Prompt å¡ç‰‡ â”€â”€â”€
    yaml_count = 0
    yaml_cards = ""
    cat_set = set()
    try:
        import yaml as _yaml
    except ImportError:
        _yaml = None
    if PROMPT_IDX.exists():
        with open(PROMPT_IDX, "r", encoding="utf-8") as f:
            idx = json.load(f)
        prompts = idx.get("prompts", [])
        yaml_count = len(prompts)
        for cat_id, info in idx.get("categories", {}).items():
            cat_set.add((cat_id, info["name"]))
        # Build detail pages + cards
        for p in prompts:
            slug = p["slug"]
            name = p["name"]
            cat_id = p["category_id"]
            cat_name = p["category"]
            author = p["author"]
            source = p["source"]
            preview = p["preview"]
            plen = p["prompt_length"]
            # Read full prompt from YAML
            yaml_path = PROMPT_YAML_DIR / p["filename"]
            full_prompt = ""
            if yaml_path.exists() and _yaml:
                try:
                    with open(yaml_path, "r", encoding="utf-8") as f:
                        data = _yaml.safe_load(f)
                    if data:
                        if "metadata" in data:
                            full_prompt = data.get("system_prompt", "")
                        elif "versions" in data and data["versions"]:
                            full_prompt = data["versions"][-1].get("template", "")
                        elif "system_prompt" in data:
                            full_prompt = data["system_prompt"]
                except Exception:
                    full_prompt = preview
            if not full_prompt:
                full_prompt = preview
            # Detail page
            prompt_esc = esc(full_prompt)
            prompt_html = f'<pre style="white-space:pre-wrap;word-break:break-word;background:#f8f8f8;padding:20px;border-radius:8px;font-size:14px;line-height:1.7;max-height:70vh;overflow-y:auto">{prompt_esc}</pre>'
            copy_js = "function cp(){const t=document.getElementById('pt').textContent;navigator.clipboard.writeText(t).then(()=>{const b=document.getElementById('cb');b.textContent='å·²è¤‡è£½!';setTimeout(()=>b.textContent='è¤‡è£½ Prompt',1500);});}"
            tags_detail = " ".join(f'<span style="display:inline-block;padding:2px 8px;background:#e8f0fe;color:#1a73e8;border-radius:10px;font-size:12px;margin-right:4px">{esc(t)}</span>' for t in p.get("tags", []))
            ab = f'''<article><h1>{esc(name)}</h1>
<div class="meta"><strong>åˆ†é¡</strong>ï¼š{esc(cat_name)} | <strong>ä½œè€…</strong>ï¼š{esc(author) if author else "ç¤¾ç¾¤è²¢ç»"} | <strong>é•·åº¦</strong>ï¼š{plen} å­—å…ƒ</div>
{f'<div class="meta"><strong>ä¾†æº</strong>ï¼š<a href="{esc(source)}" target="_blank">{esc(source)[:80]}</a></div>' if source else '<div class="meta"><strong>ä¾†æº</strong>ï¼šawesome-chatgpt-prompts ç¤¾ç¾¤æ”¶é›†</div>'}
{f'<div style="margin:8px 0">{tags_detail}</div>' if tags_detail else ''}
<div style="margin:20px 0"><button id="cb" onclick="cp()" style="padding:8px 20px;background:#1a73e8;color:#fff;border:none;border-radius:5px;cursor:pointer;font-size:14px">è¤‡è£½ Prompt</button>
<a class="dl-btn sec" href="../../prompts/roles/{quote(p["filename"])}" download style="margin-left:8px">ä¸‹è¼‰ YAML</a></div>
<div id="pt">{prompt_esc}</div>
{prompt_html}
</article>'''
            wf(SITE / "prompt" / slug / "index.html", page(name, ab, back="../../prompts.html", extra_js=copy_js))
            # Card for listing page â€” search across title + preview + tags + author
            tags = p.get("tags", [])
            tags_str = ",".join(tags)
            tags_html = " ".join(f'<span style="display:inline-block;padding:1px 6px;background:#e8f0fe;color:#1a73e8;border-radius:8px;font-size:11px;margin-right:3px">{esc(t)}</span>' for t in tags[:5])
            search_text = f"{name} {preview[:150]} {tags_str} {author} {cat_name}".lower()
            src_link = f' Â· <a href="{esc(source)}" target="_blank" style="font-size:12px">ä¾†æº</a>' if source else ""
            yaml_cards += f'''<div class="card" data-cat="{cat_id}" data-search="{esc(search_text)}">
<h3><a href="prompt/{slug}/index.html">{esc(name)}</a></h3>
<div class="meta">{esc(cat_name)} Â· {plen} å­—å…ƒ{(" Â· " + esc(author)) if author else ""}{src_link}</div>
<div style="margin:4px 0">{tags_html}</div>
<div class="meta" style="color:#999">{esc(preview[:120])}...</div></div>\n'''
    # â”€â”€â”€ Part B: å­¸ç¿’è³‡æº (é›»å­æ›¸ + ç¤¾ç¾¤è³‡æ–™) â”€â”€â”€
    RESOURCE_INFO = {
        "knowledge_base.md": ("çŸ¥è­˜ç²¾èƒåº«", "13 ç¯‡æ•™å­¸æ–‡ç«  + 42 æ”¯å½±ç‰‡é€£çµ", "Tå®¢é‚¦ã€é›»è…¦ç©ç‰©ç­‰", True),
        "url_report.md": ("AI è³‡æºé€£çµå½™æ•´", "227 YouTube + 148 ç¶²ç«™é€£çµ", "Facebook AI ç¤¾åœ˜", True),
        "compact_digest.md": ("ç¤¾ç¾¤è²¼æ–‡æ‘˜è¦ï¼ˆç²¾ç°¡ç‰ˆï¼‰", "475 æ¢ Facebook AI ç¤¾åœ˜è²¼æ–‡æ‘˜è¦", "75 å€‹ FB ç¤¾åœ˜", True),
        "rich_digest.md": ("ç¤¾ç¾¤è²¼æ–‡æ‘˜è¦ï¼ˆå®Œæ•´ç‰ˆï¼‰", "475 æ¢è²¼æ–‡å®Œæ•´ç‰ˆ", "75 å€‹ FB ç¤¾åœ˜", True),
        "related_data.md": ("Facebook AI æ•¸æ“šé›†", "583 ç­† Facebook AI ç›¸é—œè³‡æ–™", "å€‹äºº FB è³‡æ–™åŒ¯å‡º", False),
        "group_list.md": ("Facebook ç¤¾åœ˜æŒ‡å—", "75 å€‹ AI/ç§‘æŠ€ç›¸é—œç¤¾åœ˜", "ç”¨æˆ¶è¨‚é–±ç¤¾åœ˜", True),
        "ebooks/prompt_bible.md": ("The AI Prompt Bible", "å®Œæ•´é›»å­æ›¸ â€” AI æç¤ºè©è–ç¶“", "Anton Volney è‘—", True),
        "ebooks/prompt_handbook.md": ("AI Prompt Engineering Handbook", "æç¤ºå·¥ç¨‹æ‰‹å†Šï¼ˆè‹±æ–‡ç‰ˆï¼‰", "Roman Lahinouski è‘—", True),
        "ebooks/prompt_guide.md": ("The Ultimate AI Prompt Engineering Guide", "çµ‚æ¥µæç¤ºå·¥ç¨‹æŒ‡å—", "Ink è‘—", True),
        "ebooks/prompt_handbook_zh.md": ("AI æç¤ºå·¥ç¨‹æ‰‹å†Šï¼ˆä¸­æ–‡ç‰ˆï¼‰", "ä¸­æ–‡ç¿»è­¯ç‰ˆ", "Roman Lahinouski è‘—, ä¸­è­¯", True),
    }
    resource_cards = ""
    resource_rendered = 0
    for rslug, (rtitle, rdesc, rsource, render) in RESOURCE_INFO.items():
        src_file = PROMPT_DIR / rslug
        if not src_file.exists():
            continue
        sz = src_file.stat().st_size
        sz_s = f"{sz/1024:.0f}KB" if sz < 1048576 else f"{sz/1048576:.1f}MB"
        ps = rslug.replace("/", "_").replace(".md", "")
        if render:
            with open(src_file, "r", encoding="utf-8") as f:
                text = f.read()
            ch = md2html(text)
            ab = f'<article><h1>{esc(rtitle)}</h1><div class="meta"><strong>ä¾†æº</strong>ï¼š{esc(rsource)} | <strong>å¤§å°</strong>ï¼š{sz_s}</div><div class="content">{ch}</div><div style="margin-top:25px;padding-top:20px;border-top:1px solid #eee"><a class="dl-btn" href="../../../prompts/{quote(rslug)}" download>ä¸‹è¼‰åŸå§‹æª”</a></div></article>'
            wf(SITE / "prompt" / ps / "index.html", page(rtitle, ab, back="../../prompts.html"))
            resource_rendered += 1
            resource_cards += f'<div class="card"><h3><a href="prompt/{ps}/index.html">{esc(rtitle)}</a></h3><div class="meta">{esc(rdesc)} Â· {sz_s}</div><div class="meta">ä¾†æºï¼š{esc(rsource)}</div><div class="dl"><a href="prompt/{ps}/index.html">é–±è®€</a> <a href="../prompts/{quote(rslug)}" download>ä¸‹è¼‰</a></div></div>\n'
        else:
            resource_cards += f'<div class="card"><h3>{esc(rtitle)}</h3><div class="meta">{esc(rdesc)} Â· {sz_s}</div><div class="dl"><a href="../prompts/{quote(rslug)}" download>ä¸‹è¼‰ï¼ˆ{sz_s}ï¼‰</a></div></div>\n'
    # â”€â”€â”€ Build listing page â”€â”€â”€
    cat_btns = '<button class="tg-btn active" onclick="fp(this,\'\')">å…¨éƒ¨</button>\n'
    for cid, cname in sorted(cat_set, key=lambda x: x[1]):
        cat_btns += f'<button class="tg-btn" onclick="fp(this,\'{cid}\')">{cname}</button>\n'
    filter_js = """let cc='';function fp(b,c){cc=c;document.querySelectorAll('.tg-btn').forEach(x=>x.classList.remove('active'));b.classList.add('active');ff();}
function ff(){const q=document.getElementById('pq').value.toLowerCase();const words=q.split(/\\s+/).filter(w=>w);document.querySelectorAll('#pcards .card').forEach(c=>{const s=c.dataset.search||'';const catOk=!cc||c.dataset.cat===cc;const searchOk=!words.length||words.every(w=>s.includes(w));c.classList.toggle('hidden',!(catOk&&searchOk));});}"""
    body = f"""<header><h1>\U0001f916 Prompt \u5eab</h1>
<p>662 \u500b AI \u63d0\u793a\u8a5e\u89d2\u8272 + \u5b78\u7fd2\u8cc7\u6e90\u8207\u96fb\u5b50\u66f8</p>
<div class="stats"><span class="stat">{yaml_count} \u500b Prompt \u89d2\u8272</span><span class="stat">{len(cat_set)} \u500b\u5206\u985e</span><span class="stat">\u542b 4 \u672c\u96fb\u5b50\u66f8</span></div></header>
<h2 class="sec-title">Prompt \u89d2\u8272\u5eab</h2>
<input class="search" type="text" id="pq" placeholder="æœå°‹ Promptï¼ˆåç¨±ã€å…§å®¹ã€æ¨™ç±¤ã€ä½œè€…ï¼‰..." oninput="ff()">
<div class="tg-btns">{cat_btns}</div>
<div id="pcards">{yaml_cards}</div>
<h2 class="sec-title">\u5b78\u7fd2\u8cc7\u6e90</h2>
{resource_cards}"""
    wf(SITE / "prompts.html", page("Prompt \u5eab", body, back="index.html", extra_js=filter_js))
    log.info(f"\u5efa\u7f6e: Prompt \u5eab ({yaml_count} YAML + {resource_rendered} \u8cc7\u6e90\u9801)")
    return yaml_count + len(RESOURCE_INFO)

def build_tips():
    if not TIPS_STATE.exists():
        log.warning("AI Tips: ç‹€æ…‹æª”ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œ fetch_ai_tips.py")
        wf(SITE / "ai-tips.html", page("AI ä½¿ç”¨æŠ€å·§",
            '<header><h1>\U0001f4a1 AI ä½¿ç”¨æŠ€å·§</h1><p>è«‹å…ˆåŸ·è¡Œ fetch_ai_tips.py</p></header>',
            back="index.html"))
        return 0
    with open(TIPS_STATE, "r", encoding="utf-8") as f:
        state = json.load(f)
    articles = [a for a in state["articles"] if a["status"] == "fetched"]
    articles.sort(key=lambda x: x.get("pub_time", ""), reverse=True)
    cards = []
    total_imgs = 0
    for art in articles:
        slug = art.get("slug", "")
        if not slug:
            continue
        md_file = TIPS_DIR / slug / "index.md"
        if not md_file.exists():
            continue
        with open(md_file, "r", encoding="utf-8") as f:
            md_text = f.read()
        ch = md2html(md_text)
        # Fix image paths
        ch = ch.replace('src="assets/', f'src="../../ai_tips/{slug}/assets/')
        # Remove the H1 + metadata blockquote + hr from converted content
        ch = re.sub(r'<h1>.*?</h1>\s*<blockquote>.*?</blockquote>\s*<hr\s*/?>', '', ch, count=1, flags=re.DOTALL)
        title = art.get("real_title", art.get("title", "untitled"))
        author = art.get("author", "é›»è…¦ç‹é˜¿é”")
        pub_time = art.get("pub_time", "")
        url = art.get("url", "")
        img_count = art.get("img_count", 0)
        total_imgs += img_count
        # Detail page
        ab = f'''<article><h1>{esc(title)}</h1>
<div class="meta"><strong>ä½œè€…</strong>ï¼š{esc(author)} | <strong>ç™¼å¸ƒæ™‚é–“</strong>ï¼š{pub_time}<br>
<a href="{url}" target="_blank">æŸ¥çœ‹åŸæ–‡ï¼ˆé›»è…¦ç‹é˜¿é”ï¼‰</a></div>
<div class="content">{ch}</div>
<div style="margin-top:25px;padding-top:20px;border-top:1px solid #eee">
<a class="dl-btn" href="../../ai_tips/{quote(slug)}/index.md" download>ä¸‹è¼‰ Markdown</a>
<a class="dl-btn sec" href="{url}" target="_blank">åŸæ–‡é€£çµ</a></div></article>'''
        wf(SITE / "tip" / slug / "index.html", page(title, ab, back="../../ai-tips.html"))
        cards.append(f'<div class="card"><h3><a href="tip/{slug}/index.html">{esc(title)}</a></h3>'
            f'<div class="meta">{pub_time} | {esc(author)} | {img_count} å¼µåœ–ç‰‡</div>'
            f'<div class="dl"><a href="tip/{slug}/index.html">é–±è®€</a>'
            f' <a href="{url}" target="_blank">åŸæ–‡</a>'
            f' <a href="../ai_tips/{quote(slug)}/index.md" download>MD</a></div></div>')
    body = f"""<header><h1>\U0001f4a1 AI ä½¿ç”¨æŠ€å·§</h1>
<p>AI å·¥å…·æ•™å­¸æ–‡ç« ï¼ˆé›»è…¦ç‹é˜¿é” kocpc.com.twï¼‰</p>
<div class="stats"><span class="stat">{len(articles)} ç¯‡æ–‡ç« </span><span class="stat">{total_imgs} å¼µåœ–ç‰‡</span></div></header>
<p style="margin-bottom:15px;color:#666;font-size:14px">ä¾†æºï¼šé›»è…¦ç‹é˜¿é”ï¼ˆkocpc.com.twï¼‰ï¼Œæ¶µè“‹ NotebookLMã€ChatGPTã€Gemini ç­‰ AI å·¥å…·æ•™å­¸</p>
{"".join(cards)}"""
    wf(SITE / "ai-tips.html", page("AI ä½¿ç”¨æŠ€å·§", body, back="index.html"))
    log.info(f"å»ºç½®: AI ä½¿ç”¨æŠ€å·§ ({len(articles)} ç¯‡, {total_imgs} åœ–)")
    return len(articles)

# â•â•â•â•â•â•â•â• Community AI â•â•â•â•â•â•â•â•

# AI åš´æ ¼é—œéµå­—ï¼ˆäºŒæ¬¡é©—è­‰ç”¨ï¼‰
_AI_KW = [
    'claude', 'gemini', 'gpt', 'chatgpt', 'openai', 'deepseek', 'llm',
    'notebooklm', 'prompt', 'agent', 'copilot', 'cursor', 'antigravity',
    'anthropic', 'codex', 'whisper', 'stable diffusion', 'midjourney',
    'comfyui', 'langchain', 'rag', 'embedding', 'fine-tune', 'lora',
    'transformer', 'github.com', 'huggingface', 'arxiv',
    'clawdbot', 'openclaw', 'coding', 'vibe cod', 'ai agent',
    'deepwiki', 'mcp', 'sdk',
    'äººå·¥æ™º', 'æ·±åº¦å­¸', 'æ©Ÿå™¨å­¸', 'å¤§æ¨¡å‹', 'èªè¨€æ¨¡å‹',
]
_JUNK_URL = ['support.weixin.qq.com', 'wx.qlogo.cn', 'mmbiz.qpic.cn',
             'dldir1.qq.com', 'res.wx.qq.com']

def _extract_community_highlights(msgs):
    """å¾åŸå§‹ AI è¨Šæ¯ä¸­æå–æœ‰åƒ¹å€¼çš„æ–‡å­—è¨è«–"""
    out = []
    for m in msgs:
        d = m['display']
        if d.startswith('[åœ–ç‰‡]') or d.startswith('[èªéŸ³]') or d.startswith('[å½±ç‰‡]'):
            continue
        if '<?xml' in d or '<appmsg' in d or d.lstrip().startswith('<msg>'):
            tm = re.search(r'<title>(.*?)</title>', d)
            d = tm.group(1).strip() if tm and len(tm.group(1).strip()) > 3 else ''
            if not d:
                continue
        d = re.sub(r'^(wxid_\w+|F\d+):\s*', '', d).strip()
        if '<msg>' in d or '<appmsg' in d:
            tm = re.search(r'<title>(.*?)</title>', d)
            d = tm.group(1).strip() if tm and len(tm.group(1).strip()) > 3 else ''
            if not d:
                continue
        if len(d) < 50:
            continue
        text_lower = (d + ' '.join(m.get('urls', []))).lower()
        if not any(kw in text_lower for kw in _AI_KW):
            continue
        # æ¸…ç† URLs
        urls = []
        seen = set()
        for u in m.get('urls', []):
            if any(j in u for j in _JUNK_URL):
                continue
            key = re.split(r'[?&]', u)[0]
            if key not in seen:
                seen.add(key)
                urls.append(u)
        out.append({'time': m['time'], 'sender': m['sender'], 'display': d[:500], 'urls': urls[:2]})
    return out


def build_community_ai():
    if not COMM_REPOS.exists():
        log.warning("Community AI: github_repos.json ä¸å­˜åœ¨")
        wf(SITE / "community-ai.html", page("AI å¿ƒå¾—ç¤¾ç¾¤åˆ†äº«",
            '<header><h1>ğŸ’¬ AI å¿ƒå¾—ç¤¾ç¾¤åˆ†äº«</h1><p>è³‡æ–™å°šæœªæº–å‚™</p></header>',
            back="index.html"))
        return 0
    with open(COMM_REPOS, "r", encoding="utf-8") as f:
        repos = json.load(f)
    # è®€å–æ–‡å­—è¨è«–
    highlights = []
    if COMM_MSGS.exists():
        with open(COMM_MSGS, "r", encoding="utf-8") as f:
            raw_msgs = json.load(f)
        highlights = _extract_community_highlights(raw_msgs)

    # â”€â”€ GitHub è¡¨æ ¼ â”€â”€
    cats = {
        'AI Agent / Coding': ['claude', 'agent', 'copilot', 'cursor', 'code',
            'claw', 'skill', 'hook', 'proxy', 'bridge', 'orchestra'],
        'AI æ‡‰ç”¨ / å·¥å…·': ['ai', 'draw', 'sleep', 'yolo', 'markitdown',
            'stitch', 'clipper', 'threat', 'telegram'],
        'AI ç ”ç©¶ / ç†è«–': ['wfgy', 'embedding'],
        'é–‹ç™¼è³‡æº / API': ['public-api', 'pageindex', 'assignment', 'quant', 'rss'],
    }
    def _cat(r):
        t = (r['url'] + ' ' + r.get('description', '') + ' ' + r.get('context', '')).lower()
        for c, kws in cats.items():
            if any(k in t for k in kws):
                return c
        return 'å…¶ä»–'

    by_cat = {}
    for r in repos:
        if r.get('kind') not in ('repo', 'gist'):
            continue
        by_cat.setdefault(_cat(r), []).append(r)

    repo_html = []
    for cat_name in ['AI Agent / Coding', 'AI æ‡‰ç”¨ / å·¥å…·', 'AI ç ”ç©¶ / ç†è«–', 'é–‹ç™¼è³‡æº / API', 'å…¶ä»–']:
        items = by_cat.get(cat_name, [])
        if not items:
            continue
        items.sort(key=lambda x: x.get('stars', 0), reverse=True)
        repo_html.append(f'<h3 class="sec-title">{esc(cat_name)}</h3>')
        repo_html.append('<table><thead><tr><th>Repo</th><th>èªªæ˜</th>'
                         '<th>â˜…</th><th>èªè¨€</th><th>æ—¥æœŸ</th></tr></thead><tbody>')
        for r in items:
            desc = esc(r.get('description', '') or r.get('context', '')[:60])[:80]
            stars = r.get('stars', '')
            lang = esc(r.get('language', '') or '')
            name = f"{r['owner']}/{r['name']}" if r['kind'] == 'repo' else f"gist/{r['owner']}"
            repo_html.append(
                f'<tr><td><a href="{esc(r["url"])}" target="_blank">{esc(name)}</a></td>'
                f'<td>{desc}</td><td>{stars}</td><td>{lang}</td><td>{r["date"][:10]}</td></tr>')
        repo_html.append('</tbody></table>')

    # â”€â”€ æ–‡å­—è¨è«–ç²¾è¯ â”€â”€
    disc_html = []
    if highlights:
        disc_html.append(f'<h3 class="sec-title">æ–‡å­—è¨è«–ç²¾è¯ï¼ˆ{len(highlights)} å‰‡ï¼‰</h3>')
        for h in highlights[:60]:
            display = esc(h['display'][:300])
            disc_html.append(
                f'<div class="card"><div class="meta">{h["time"]} | {esc(h["sender"])}</div>'
                f'<p style="font-size:14px;line-height:1.7">{display}</p>')
            for u in h['urls'][:2]:
                disc_html.append(f'<div class="dl"><a href="{esc(u)}" target="_blank">{esc(u[:80])}</a></div>')
            disc_html.append('</div>')

    total_repos = sum(len(v) for v in by_cat.values())
    body = f"""<header><h1>ğŸ’¬ AI å¿ƒå¾—ç¤¾ç¾¤åˆ†äº«</h1>
<p>ä¸‰å…„å¼Ÿç¾¤ WeChat ç¾¤çµ„çš„ GitHub / AI è³‡è¨Šå½™æ•´</p>
<div class="stats"><span class="stat">{total_repos} å€‹ GitHub Repo</span>
<span class="stat">{len(highlights)} å‰‡è¨è«–</span>
<span class="stat">2025-12 ~ 2026-02</span></div></header>
<input class="search" type="text" placeholder="æœå°‹ repo æˆ–è¨è«–å…§å®¹â€¦" oninput="
var q=this.value.toLowerCase();
document.querySelectorAll('table tbody tr, .card').forEach(function(el){{
  el.style.display=el.textContent.toLowerCase().includes(q)?'':'none'}})">
<h2 style="margin:20px 0 10px">GitHub è³‡æºç´¢å¼•</h2>
{"".join(repo_html)}
<h2 style="margin:30px 0 10px">ç¤¾ç¾¤ AI è¨è«–</h2>
{"".join(disc_html)}"""

    extra_css = """
table{border-collapse:collapse;width:100%;margin-bottom:20px;font-size:14px}
th,td{border:1px solid #e0e0e0;padding:8px 12px;text-align:left}
th{background:#f5f5f5;font-weight:600}
tr:hover{background:#f8f9fa}
td a{word-break:break-all}
"""
    wf(SITE / "community-ai.html", page("AI å¿ƒå¾—ç¤¾ç¾¤åˆ†äº«", body, back="index.html", extra_css=extra_css))
    log.info(f"å»ºç½®: AI å¿ƒå¾—ç¤¾ç¾¤åˆ†äº« ({total_repos} repos, {len(highlights)} è¨è«–)")
    return total_repos


# â•â•â•â•â•â•â•â• Main â•â•â•â•â•â•â•â•

def main():
    log.info("=" * 50)
    log.info("YanYu \u77e5\u8b58\u5eab\u5efa\u7f6e\u958b\u59cb")
    log.info("=" * 50)
    SITE.mkdir(parents=True, exist_ok=True)
    # Phase 1
    log.info("-- Phase 1: \u6e96\u5099\u5167\u5bb9 --")
    eb_files, scan_only = prepare_judgments()
    pf = prepare_prompts()
    # Phase 2
    log.info("-- Phase 2: \u5efa\u7f6e\u7db2\u7ad9 --")
    wc_n, wc_img = build_wechat()
    jdg_n = build_judgments(eb_files, scan_only)
    prm_n = build_prompts(pf)
    tip_n = build_tips()
    comm_n = build_community_ai()
    build_main_page(wc_n, wc_img, jdg_n, prm_n, tip_n, comm_n)
    log.info("=" * 50)
    log.info(f"\u5b8c\u6210: \u5fae\u4fe1{wc_n} | \u5224\u6c7a{jdg_n} | Prompt{prm_n} | AI\u6280\u5de7{tip_n} | \u793e\u7fa4{comm_n}")
    log.info(f"\u65e5\u8a8c: {log_path}")

if __name__ == "__main__":
    main()
